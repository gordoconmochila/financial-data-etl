import requests
import pandas as pd
import datetime
import boto3
import psycopg2

# Configuración de la API
api_key = 'W1NZYYGF2AYLKBM1'
symbols = ['AAPL', 'GOOGL']

# Configuración de la conexión a Redshift
redshift_endpoint = "http://data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com/"
redshift_db = "data-engineer-database"
redshift_user = "josee_alberdi_coderhouse"
redshift_password = "D4tRG1SkU4"
redshift_port = "5439"

# Configuración de S3
s3_bucket_name = "nombre_de_tu_bucket"
s3_key = "financial_data.csv"

# Crear un DataFrame para almacenar todos los datos
all_data = []

# 1. Consultar la API y procesar los datos
for symbol in symbols:
    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={api_key}'
    response = requests.get(url)
    data = response.json()

    if 'Error Message' in data:
        print(f"Error en la solicitud para {symbol}: {data['Error Message']}")
    elif 'Time Series (Daily)' in data:
        time_series = data["Time Series (Daily)"]
        rows = []

        for date, daily_data in time_series.items():
            row = {
                "symbol": symbol,
                "date": date,
                "open": float(daily_data["1. open"]),
                "high": float(daily_data["2. high"]),
                "low": float(daily_data["3. low"]),
                "close": float(daily_data["4. close"]),
                "volume": float(daily_data["5. volume"]),
                "timestamp": datetime.datetime.now()
            }
            rows.append(row)

        all_data.extend(rows)
    else:
        print(f"Respuesta inesperada para {symbol}: {data}")

# 2. Guardar el DataFrame en un archivo CSV
df = pd.DataFrame(all_data)
df.to_csv('financial_data.csv', index=False)

# 3. Subir el archivo CSV a S3
s3 = boto3.client('s3')
s3.upload_file('financial_data.csv', s3_bucket_name, s3_key)

# 4. Conectar a Redshift y cargar los datos desde S3
conn = psycopg2.connect(
    dbname=redshift_db,
    user=redshift_user,
    password=redshift_password,
    host=redshift_endpoint,
    port=redshift_port
)
cur = conn.cursor()

copy_query = f"""
COPY nombre_de_tu_tabla
FROM 's3://{s3_bucket_name}/{s3_key}'
CREDENTIALS 'aws_access_key_id=TU_AWS_ACCESS_KEY_ID;aws_secret_access_key=TU_AWS_SECRET_ACCESS_KEY'
DELIMITER ','
IGNOREHEADER 1
REGION 'tu-region';
"""

cur.execute(copy_query)
conn.commit()

# Cerrar la conexión
cur.close()
conn.close()

print("Datos cargados exitosamente en Redshift.")
